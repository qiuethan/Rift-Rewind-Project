"""Compute pairwise ability similarity using OpenRouter via openrouter_helper.

This script expects `output/ability_index.json` (generated by `build_ability_index.py`).
It generates pairwise similarity scores (0.0 - 1.0) for ALL abilities in the game and writes them to
`output/ability_similarity.csv` with columns: champ1, ability1, champ2, ability2, score, explanation.

The script calls `openrouter_helper.get_openrouter_response` and instructs the LLM
to return a JSON object with `score` (float between 0 and 1) and `explanation`.

Note: For large numbers of abilities this will create MANY API calls (N*N pairs where N is ~700+).
Use `--limit` to test on a small subset while iterating.
"""
import json
from pathlib import Path
import argparse
import itertools
import csv
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import re
import os
from datetime import datetime
from dotenv import load_dotenv
from tqdm import tqdm

# Load environment variables from .env file
load_dotenv()

# Add parent directory to path to import openrouter_helper
import sys
sys.path.insert(0, str(Path(__file__).resolve().parents[1]))
from openrouter_helper import get_openrouter_response

DATA_DIR = Path(__file__).resolve().parents[1] / 'data'
OUTPUT_DIR = Path(__file__).resolve().parents[1] / 'output'
INDEX_PATH = OUTPUT_DIR / 'ability_index.json'
OUT_CSV = OUTPUT_DIR / 'ability_similarity.csv'


SYSTEM_PROMPT = (
    "You are an assistant that compares two League of Legends abilities to help newer players learn champions. The goal is to tell players which abilities feel and function similarly so they can transfer skills across champions.\n"
    "Return a JSON object ONLY with two fields:\n"
    "- explanation: a concise (1â€“2 sentences) rationale of the similarity in plain language.\n"
    "- score: a float in [0.0, 1.0] where 1.0 = nearly identical mechanics/inputs/usage.\n\n"
    "Scoring priorities (top to bottom):\n"
    "1) Targeting & Input Pattern (HIGHEST): skillshot vs targeted vs ground-targeted; linear vs circular vs cone; projectile behavior (returns, pierces, bounces); mobility class (dash vs blink), channel vs instant, toggle vs single-cast, global vs local.\n"
    "2) Primary Functional Effect (MEDIUM): crowd control, buff/debuff auras, on-hit resets/auto-attack modifiers, persistent auras vs one-off effects and more.\n"
    "Calibration guidance:\n"
    "- Returning linear skillshots (out-and-back) â‰ˆ 0.85â€“0.95 even if damage types differ.\n"
    "- Two mobility tools of the same class (dash vs dash, blink vs blink) â‰ˆ 0.65â€“0.75 even if one adds damage or utility.\n"
    "- Different input but similar purpose (e.g., jump-to-stun vs charm skillshot) â‰ˆ ~0.65.\n"
    "- Same pattern but one includes CC and the other does not â‰ˆ ~0.75 (still quite similar).\n"
    "- Similar purpose but melee vs ranged execution â‰ˆ 0.4â€“0.5.\n"
    "- Toggles/aura-like sustained damage vs point-and-click burst are fundamentally different: â‰¤ 0.2â€“0.3.\n\n"
    "Use any provided labeled examples as ground truth calibration: prefer matching their scale when a comparison is analogous.\n"
    "Ignore lore, champion identity, damage type, and raw numbers beyond how they change mechanics. Focus on how a player aims, activates, and experiences the ability.\n\n"
    "STRICT OUTPUT: Return ONLY the raw JSON object with keys 'score' (number, not a string) and 'explanation'. No markdown fences, no extra keys, no surrounding text."
)


def _format_examples_for_prompt(examples: list) -> str:
    """Format few-shot examples into a compact prompt string."""
    def _esc(s: str) -> str:
        # Escape quotes for inline JSON fragment in the prompt
        return (s or "").replace("\\", "\\\\").replace('"', '\\"')
    lines = [
        "Use the following labeled examples to calibrate your judgment. Each example shows the mechanics text and the expected JSON output.",
    ]
    for i, ex in enumerate(examples, 1):
        name1 = _esc(ex['ability1_name'])
        name2 = _esc(ex['ability2_name'])
        exp = _esc(ex['explanation'].strip())
        lines.append(
            f"Example {i}:\n"
            f"Ability 1: {ex['champ1']} {ex['ability1_type']} \"{name1}\"\n"
            f"Mechanics: {ex.get('ability1_mechanics', '').strip()}\n"
            f"Ability 2: {ex['champ2']} {ex['ability2_type']} \"{name2}\"\n"
            f"Mechanics: {ex.get('ability2_mechanics', '').strip()}\n"
            f"JSON returned by labeler: {{\"score\": {ex['score']}, \"explanation\": \"{exp}\"}}\n"
        )
    return "\n".join(lines)


def build_prompt(ability_a: dict, ability_b: dict, few_shot_context: str | None = None) -> str:
    """Build a prompt for comparing two abilities, optionally with few-shot calibration."""
    prefix = "" if not few_shot_context else (few_shot_context.strip() + "\n\n")
    prompt = (
        prefix
        + "Compare the following two abilities and return a JSON object with fields 'score' and 'explanation'.\n\n"
        + f"Ability 1:\n"
        + f"Champion: {ability_a['champion']}\n"
        + f"Type: {ability_a['type']}\n"
        + f"Name: {ability_a['name']}\n"
        + f"Description: {ability_a['description']}\n\n"
        + f"Ability 2:\n"
        + f"Champion: {ability_b['champion']}\n"
        + f"Type: {ability_b['type']}\n"
        + f"Name: {ability_b['name']}\n"
        + f"Description: {ability_b['description']}\n\n"
        + "Output 'score' in [0.0, 1.0]."
    )
    return prompt


def parse_score_from_response(text: str):
    # Clean up markdown code blocks if present
    cleaned_text = text.strip()
    if cleaned_text.startswith('```json'):
        cleaned_text = cleaned_text[7:]  # Remove ```json
    if cleaned_text.startswith('```'):
        cleaned_text = cleaned_text[3:]  # Remove ```
    if cleaned_text.endswith('```'):
        cleaned_text = cleaned_text[:-3]  # Remove trailing ```
    cleaned_text = cleaned_text.strip()
    
    # Try to parse JSON first
    try:
        obj = json.loads(cleaned_text)
        score = float(obj.get('score'))
        reason = obj.get('explanation') or obj.get('reason') or ''
        return max(0.0, min(1.0, score)), reason
    except Exception:
        # Fallback: find numeric value between 0 and 1
        m = re.search(r"0(?:\.\d+)?|1(?:\.0+)?", cleaned_text)
        if m:
            try:
                score = float(m.group(0))
                return max(0.0, min(1.0, score)), cleaned_text.strip()
            except Exception:
                pass
    return None, cleaned_text.strip()


def extract_abilities(champion_index, include_passives=False):
    """Extract all abilities from the champion index into a flat list."""
    abilities = []
    
    for champ in champion_index:
        champ_name = champ.get('name')
        # Preferred path: LLM-processed schema with unified 'abilities'
        if isinstance(champ.get('abilities'), list) and champ['abilities']:
            for ab in champ['abilities']:
                ab_type = (ab.get('type') or '').strip() or 'Ability'
                if not include_passives and ab_type.lower() == 'passive':
                    continue
                desc = (
                    (ab.get('mechanics_description') or '').strip()
                    or (ab.get('original_description') or '').strip()
                )
                if not desc:
                    continue
                abilities.append({
                    'champion': champ_name,
                    'type': ab_type,
                    'name': (ab.get('name') or '').strip() or ab_type,
                    'description': desc,
                })
            continue  # handled this champion via 'abilities'

        # Legacy path: raw schema with 'passive' and 'spells'
        # Add passive
        if include_passives:
            passive = champ.get('passive')
            if passive and passive.get('description'):
                abilities.append({
                    'champion': champ_name,
                    'type': 'Passive',
                    'name': passive.get('name') or 'Passive',
                    'description': passive.get('description')
                })

        # Add spells
        spells = champ.get('spells') or []
        spell_labels = ['Q', 'W', 'E', 'R']
        for i, spell in enumerate(spells[:4]):
            if spell.get('description'):
                abilities.append({
                    'champion': champ_name,
                    'type': spell_labels[i] if i < len(spell_labels) else f'Spell{i+1}',
                    'name': spell.get('name') or f'Spell{i+1}',
                    'description': spell.get('description')
                })
    
    return abilities


def load_failed_pairs_as_ability_tuples(failed_csv: Path, all_abilities: list) -> list:
    """Load failed pairs from CSV and match them to ability objects.
    
    Returns a list of (ability_a, ability_b) tuples.
    """
    # Create a lookup dictionary for quick ability matching
    ability_lookup = {}
    for ab in all_abilities:
        key = (ab['champion'], ab['type'], ab['name'])
        ability_lookup[key] = ab
    
    pairs = []
    with open(failed_csv, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            key1 = (row['champ1'], row['ability1_type'], row['ability1_name'])
            key2 = (row['champ2'], row['ability2_type'], row['ability2_name'])
            
            ab1 = ability_lookup.get(key1)
            ab2 = ability_lookup.get(key2)
            
            if ab1 and ab2:
                pairs.append((ab1, ab2))
            else:
                if not ab1:
                    print(f"Warning: Could not find ability {key1}")
                if not ab2:
                    print(f"Warning: Could not find ability {key2}")
    
    return pairs


def merge_with_original(failed_csv: Path, retry_results_csv: Path, retry_results: list):
    """Merge retry results back into the original comparison file.
    
    Reads the original file that was used to generate the failed pairs,
    replaces failed rows with retry results, and overwrites the original.
    """
    # Get the original file path from the failed CSV parent directory
    original_csv = failed_csv.parent / 'final_comparisons_20251107_194606'
    
    if not original_csv.exists():
        print(f"Warning: Original file not found at {original_csv}")
        print(f"Retry results saved to {retry_results_csv}")
        return
    
    # Create a lookup for retry results
    retry_lookup = {}
    for row in retry_results:
        key = (row[0], row[1], row[2], row[3], row[4], row[5])  # champ1, ability1_type, ability1_name, champ2, ability2_type, ability2_name
        retry_lookup[key] = row
    
    print(f"Created lookup with {len(retry_lookup)} retry results")
    
    # Read original file and replace failed rows
    merged_rows = []
    replaced_count = 0
    kept_count = 0
    
    with open(original_csv, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        header = next(reader)
        merged_rows.append(header)
        
        for row in reader:
            if len(row) < 8:
                merged_rows.append(row)
                kept_count += 1
                continue
                
            key = (row[0], row[1], row[2], row[3], row[4], row[5])
            
            # Check if this is a failed row that we have a retry result for
            if key in retry_lookup:
                merged_rows.append(retry_lookup[key])
                replaced_count += 1
            else:
                merged_rows.append(row)
                kept_count += 1
    
    # Write back to original file
    with open(original_csv, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerows(merged_rows)
    
    print(f"âœ… Merged results back into {original_csv}")
    print(f"   Total rows: {len(merged_rows) - 1}")  # -1 for header
    print(f"   Replaced with retry results: {replaced_count}")
    print(f"   Kept from original: {kept_count}")


def compute_ability_pairwise(index, out_csv_path: Path, model_name: str, num_threads: int = 4, limit: int = None, include_passives: bool = False, retry_failed_csv: Path = None):
    champions = index
    if limit:
        champions = champions[:limit]

    # Extract all abilities
    abilities = extract_abilities(champions, include_passives=include_passives)
    print(f"Extracted {len(abilities)} abilities from {len(champions)} champions")
    
    # Generate pairs based on mode
    retry_mode = False
    if retry_failed_csv and retry_failed_csv.exists():
        # Retry mode: load specific failed pairs
        retry_mode = True
        print(f"\nðŸ”„ RETRY MODE: Loading failed pairs from {retry_failed_csv}")
        pairs = load_failed_pairs_as_ability_tuples(retry_failed_csv, abilities)
        print(f"Loaded {len(pairs)} failed pairs to retry")
    else:
        # Normal mode: generate all pairs, excluding abilities from the same champion
        all_pairs = list(itertools.combinations(abilities, 2))
        pairs = [(a, b) for a, b in all_pairs if a['champion'] != b['champion']]
        print(f"Computing similarities for {len(pairs)} ability pairs (excluding same-champion comparisons)")

    out_csv_path.parent.mkdir(parents=True, exist_ok=True)
    
    # In retry mode, we'll collect results and merge at the end
    if retry_mode:
        # Store results temporarily
        retry_results = []
    
    # In retry mode, we'll collect results and merge at the end
    if retry_mode:
        # Store results temporarily
        retry_results = []
    
    with open(out_csv_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['champ1', 'ability1_type', 'ability1_name', 'champ2', 'ability2_type', 'ability2_name', 'score', 'explanation'])

        # Prepare few-shot context if available (injected later from main)
        few_shot_context = getattr(compute_ability_pairwise, "_few_shot_context", None)

        def task(pair):
            a, b = pair
            prompt = build_prompt(a, b, few_shot_context=few_shot_context)
            try:
                resp = get_openrouter_response(model_name, prompt, SYSTEM_PROMPT)
            except Exception as e:
                return (
                    a['champion'], a['type'], a['name'],
                    b['champion'], b['type'], b['name'],
                    None, f'Error: {e}'
                )

            score, reason = parse_score_from_response(resp)
            if score is None:
                # Try to extract a float with regex more aggressively
                m = re.search(r"(0(?:\.\d+)?|1(?:\.0+)?)", resp)
                try:
                    score_val = float(m.group(0)) if m else None
                except Exception:
                    score_val = None
                return (
                    a['champion'], a['type'], a['name'],
                    b['champion'], b['type'], b['name'],
                    score_val, reason
                )
            return (
                a['champion'], a['type'], a['name'],
                b['champion'], b['type'], b['name'],
                score, reason
            )

        with ThreadPoolExecutor(max_workers=num_threads) as ex:
            futures = {ex.submit(task, p): p for p in pairs}
            
            # Use tqdm for progress tracking
            with tqdm(total=len(pairs), desc="Computing ability similarities", unit="pair") as pbar:
                for fut in as_completed(futures):
                    champ1, ability1_type, ability1_name, champ2, ability2_type, ability2_name, score, explanation = fut.result()
                    row = [
                        champ1, ability1_type, ability1_name,
                        champ2, ability2_type, ability2_name,
                        '' if score is None else score, explanation
                    ]
                    
                    if retry_mode:
                        retry_results.append(row)
                    else:
                        writer.writerow(row)
                        # flush incrementally
                        f.flush()
                    
                    pbar.update(1)
                    time.sleep(0.05)

    # In retry mode, merge with original file
    if retry_mode:
        print(f"\nðŸ“ Merging retry results with original file...")
        merge_with_original(retry_failed_csv, out_csv_path, retry_results)
    else:
        print(f"\nâœ“ Wrote ability pairwise similarity CSV to {out_csv_path}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Compute pairwise ability similarity using OpenRouter')
    parser.add_argument('--model', default='google/gemini-2.5-flash-lite', help='Model name to use')
    parser.add_argument('--index', default=str(INDEX_PATH), help='Path to ability_index.json')
    parser.add_argument('--out', default=str(OUT_CSV), help='Output CSV path')
    parser.add_argument('--num-threads', type=int, default=4, help='Number of concurrent API calls')
    parser.add_argument('--limit', type=int, default=0, help='Limit number of champions (for testing). 0 means no limit')
    parser.add_argument('--include-passives', action='store_true', help='Include passive abilities in comparisons (default: False)')
    parser.add_argument('--retry-failed', type=str, default='', help='Path to CSV with failed pairs to retry (only processes these specific pairs)')
    args = parser.parse_args()

    # Add timestamp to output filename
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    out_path = Path(args.out)
    out_path_with_timestamp = out_path.parent / f"{out_path.stem}_{timestamp}{out_path.suffix}"

    with open(args.index, 'r', encoding='utf-8') as fh:
        index = json.load(fh)

    limit = args.limit if args.limit and args.limit > 0 else None
    
    # Handle retry mode
    retry_failed_path = Path(args.retry_failed) if args.retry_failed else None
    if retry_failed_path and not retry_failed_path.exists():
        print(f"Error: Retry failed CSV not found: {retry_failed_path}")
        exit(1)
    
    # Inject few-shot examples if available (hardcoded behavior)
    FEW_SHOT_CSV = OUTPUT_DIR / 'mechanics_similarity_labels_enhanced.csv'
    FEW_SHOT_MAX = 8

    def _load_few_shot_examples(csv_path: Path, max_examples: int = 8):
        examples = []
        if not csv_path.exists():
            return examples
        try:
            with open(csv_path, 'r', encoding='utf-8') as cf:
                reader = csv.DictReader(cf)
                for row in reader:
                    try:
                        score = float(row.get('score')) if row.get('score') not in (None, '') else None
                    except Exception:
                        score = None
                    if score is None:
                        continue
                    examples.append({
                        'champ1': row.get('champ1', '').strip(),
                        'ability1_type': row.get('ability1_type', '').strip(),
                        'ability1_name': row.get('ability1_name', '').strip(),
                        'champ2': row.get('champ2', '').strip(),
                        'ability2_type': row.get('ability2_type', '').strip(),
                        'ability2_name': row.get('ability2_name', '').strip(),
                        'score': score,
                        'explanation': (row.get('explanation') or '').strip(),
                        'ability1_mechanics': (row.get('ability1_mechanics') or '').strip(),
                        'ability2_mechanics': (row.get('ability2_mechanics') or '').strip(),
                    })
                    if len(examples) >= max_examples:
                        break
        except Exception:
            return []
        return examples

    few_shot_context = None
    if FEW_SHOT_CSV.exists():
        exs = _load_few_shot_examples(FEW_SHOT_CSV, max_examples=FEW_SHOT_MAX)
        if exs:
            few_shot_context = _format_examples_for_prompt(exs)
    # attach to function attribute for access in worker threads (None if no examples)
    setattr(compute_ability_pairwise, "_few_shot_context", few_shot_context)

    compute_ability_pairwise(index, out_path_with_timestamp, args.model, num_threads=args.num_threads, limit=limit, include_passives=args.include_passives, retry_failed_csv=retry_failed_path)
